# ğŸ§  The Memory Twin

## ğŸ“‹ Resumen

Memory Twin es un sistema de **memoria episÃ³dica inteligente** que se integra con tu asistente de IA (Copilot, Cursor, Claude) para evitar la "amnesia tÃ©cnica". Utiliza **modelos de lenguaje avanzados (LLMs)** y **bases de datos vectoriales** para capturar, estructurar y recuperar el razonamiento detrÃ¡s de cada decisiÃ³n de cÃ³digo, permitiendo que tu equipo aprenda de errores pasados y reutilice soluciones exitosas automÃ¡ticamente.

---

## ğŸ¤– Procesamiento de Lenguaje Natural (PLN)

El corazÃ³n de Memory Twin es un pipeline sofisticado de PLN diseÃ±ado para transformar texto no estructurado (pensamientos de IA) en conocimiento consultable.

### ğŸ”„ Pipeline de Procesamiento

```mermaid
graph TD
    A[Input: Raw Thinking] -->|EstructuraciÃ³n: Gemini 2.0| B(Episodio JSON)
    B -->|Embedding: all-MiniLM| C[Vector Store: ChromaDB]
    B -->|Almacenamiento| D[Metadata Store: SQLite]
    C -->|Clustering: DBSCAN| E[DetecciÃ³n de Patrones]
    E -->|SÃ­ntesis: Gemini 2.0| F[Meta-Memorias]
    G[Consulta Usuario] -->|Embedding| H[BÃºsqueda SemÃ¡ntica]
    H -->|RAG + Contexto| I[Respuesta OrÃ¡culo]
```

### ğŸ§  Modelos y Especificaciones

| Componente | Modelo / Algoritmo | Especificaciones TÃ©cnicas | FunciÃ³n |
|------------|-------------------|---------------------------|---------|
| **EstructuraciÃ³n** | `gemini-2.0-flash` | Temp: 0.3, JSON Mode | Convierte texto libre en JSON estructurado con taxonomÃ­a definida. |
| **Embeddings** | `all-MiniLM-L6-v2` | 384 dimensiones, Max seq: 256 | Genera representaciones vectoriales densas para bÃºsqueda semÃ¡ntica. |
| **Clustering** | `DBSCAN` | `eps=0.5`, `min_samples=3` | Agrupa episodios similares sin necesitar nÃºmero de clusters predefinido. |
| **SÃ­ntesis** | `gemini-2.0-flash` | Temp: 0.4, Context Window: 1M | Consolida clusters de episodios en "Meta-Memorias" (lecciones aprendidas). |
| **RAG** | HÃ­brido | Top-k: 5, Threshold: 0.7 | RecuperaciÃ³n semÃ¡ntica + filtrado por metadatos (proyecto, tags). |

### ğŸ§© Detalles de ImplementaciÃ³n

1.  **Embeddings & Similitud SemÃ¡ntica**:
    Utilizamos `sentence-transformers/all-MiniLM-L6-v2` por su excelente balance velocidad/precisiÃ³n (14,200 sentencias/seg). La similitud se calcula mediante **distancia coseno** en un espacio de 384 dimensiones.
    *   *Umbral de relevancia*: Los resultados con similitud < 0.4 son descartados para reducir alucinaciones.

2.  **RAG (Retrieval-Augmented Generation)**:
    El motor `Oraculo` no solo busca texto; inyecta contexto estructurado en el prompt del sistema.
    *   *Prompt Engineering*: Se utiliza un prompt dinÃ¡mico que prioriza **Meta-Memorias** (conocimiento consolidado) sobre **Episodios** individuales para dar respuestas mÃ¡s generalizables.

3.  **Clustering de Memorias (ConsolidaciÃ³n)**:
    Implementamos un proceso inspirado en la consolidaciÃ³n del sueÃ±o humano.
    *   Se calculan matrices de distancia entre todos los episodios no consolidados.
    *   `DBSCAN` identifica grupos densos de decisiones similares.
    *   El LLM analiza el cluster y extrae: *PatrÃ³n ComÃºn*, *Lecciones Aprendidas* y *Anti-patrones*.
    *   Se genera un `coherence_score` (0.0-1.0) para validar la calidad del agrupamiento.

---

## âš–ï¸ JustificaciÃ³n del Uso de PLN

Â¿Por quÃ© usar modelos complejos en lugar de una bÃºsqueda simple?

### Comparativa de TecnologÃ­as

| CaracterÃ­stica | BÃºsqueda de Texto (grep/SQL) | BÃºsqueda por Palabras Clave (Elasticsearch) | **Memory Twin (PLN SemÃ¡ntico)** |
|----------------|------------------------------|---------------------------------------------|---------------------------------|
| **ComprensiÃ³n** | Nula (solo coincidencia exacta) | Baja (sinÃ³nimos bÃ¡sicos) | **Alta** (entiende intenciÃ³n y contexto) |
| **Contexto** | Ignorado | Limitado | **Capturado** (relaciÃ³n entre archivos y decisiones) |
| **Resiliencia** | Falla con typos o sinÃ³nimos | Moderada | **Alta** (ej: "auth" â‰ˆ "login" â‰ˆ "JWT") |
| **Inferencia** | Ninguna | Ninguna | **DeducciÃ³n** de lecciones y patrones |
| **Latencia** | < 1ms | ~10ms | ~200ms (aceptable para este caso de uso) |

### ğŸ’¡ Casos de Uso donde PLN es Superior

1.  **BÃºsqueda de Conceptos Abstractos**:
    *   *Query*: "Â¿Por quÃ© elegimos esta arquitectura?"
    *   *Keyword Search*: Falla si no existe la palabra exacta "arquitectura" en los logs.
    *   *PLN*: Encuentra episodios sobre "diseÃ±o de sistema", "patrones", "estructura", aunque no usen la palabra exacta.

2.  **DetecciÃ³n de Contradicciones**:
    *   El sistema puede identificar que la "SoluciÃ³n A" en el episodio 5 contradice la "LecciÃ³n Aprendida" en el episodio 20, algo imposible con regex.

3.  **SÃ­ntesis de InformaciÃ³n**:
    *   En lugar de devolver 10 logs crudos, el sistema *lee* los 10 logs y genera un resumen coherente ("En 3 ocasiones intentamos X y fallÃ³ por Y").

### ğŸ“‰ Limitaciones y Trade-offs

*   **Latencia**: La generaciÃ³n de embeddings y la inferencia LLM aÃ±aden latencia (~500ms - 2s). *MitigaciÃ³n*: CachÃ© agresivo y procesamiento asÃ­ncrono en background.
*   **Coste**: Requiere llamadas a API (Gemini). *MitigaciÃ³n*: Uso de modelos Flash (muy econÃ³micos) y embeddings locales (coste cero).
*   **Alucinaciones**: Riesgo inherente a los LLMs. *MitigaciÃ³n*: RAG estricto (grounding) y citas de fuentes en las respuestas.

---

## ğŸš€ InstalaciÃ³n Simplificada

Memory Twin estÃ¡ diseÃ±ado para instalarse **una sola vez** en tu sistema y usarse en **mÃºltiples proyectos**.

### MÃ©todo Recomendado: `pipx` (Global)

Ideal para usar la CLI (`mt`) desde cualquier lugar sin ensuciar tus entornos virtuales.

```bash
# 1. Instalar pipx (si no lo tienes)
python -m pip install --user pipx
python -m pipx ensurepath

# 2. Instalar Memory Twin globalmente
pipx install memorytwin
```

### MÃ©todo Alternativo: `venv` (Por proyecto)

```bash
python -m venv .venv
source .venv/bin/activate  # o .venv\Scripts\activate en Windows
pip install memorytwin
```

### InstalaciÃ³n con Interfaz Web (Opcional)

Si deseas usar la interfaz grÃ¡fica (`mt oraculo`), necesitas instalar las dependencias extra:

**Con pipx:**
```bash
pipx install "memorytwin[ui]"
```

**Con pip:**
```bash
pip install "memorytwin[ui]"
```

---

## âš¡ Uso RÃ¡pido (5 Minutos)

### Paso 1: Setup en tu Proyecto
Navega a la carpeta de tu proyecto (cualquier lenguaje: Python, JS, Rust...) e inicializa Memory Twin.

```bash
cd ~/mi-proyecto-increible
mt setup
```

Esto crearÃ¡ una carpeta `data/` (ignorada por git) y un archivo `.env`.

> **Nota para proyectos existentes**: `mt setup` es **seguro** y no sobrescribirÃ¡ tus archivos.
> *   Si ya tienes `.gitignore`, el comando aÃ±adirÃ¡ las reglas necesarias automÃ¡ticamente.
> *   Si ya tienes `.env`, **no se modificarÃ¡**: deberÃ¡s aÃ±adir manualmente las variables `GOOGLE_API_KEY` o `OPENROUTER_API_KEY`.

### Paso 2: ConfiguraciÃ³n
Abre el archivo `.env` generado y configura tu proveedor de LLM.

#### OpciÃ³n A: OpenRouter (recomendado - acceso a mÃºltiples modelos gratuitos)
```ini
OPENROUTER_API_KEY=tu_api_key_aqui
LLM_PROVIDER=openrouter
LLM_MODEL=amazon/nova-2-lite-v1:free
```

> **Modelos gratuitos recomendados en OpenRouter** (Dic 2025):
> - `amazon/nova-2-lite-v1:free` - 1M contexto, rÃ¡pido
> - `qwen/qwen3-coder:free` - 262K contexto, excelente para cÃ³digo
> - `tngtech/deepseek-r1t-chimera:free` - 164K contexto, razonamiento

#### OpciÃ³n B: Google Gemini
```ini
GOOGLE_API_KEY=tu_api_key_aqui
LLM_PROVIDER=google
LLM_MODEL=gemini-2.0-flash
```

### Paso 3: GestiÃ³n Visual (OrÃ¡culo)
Para explorar tus memorias de forma visual, lanza la interfaz web:

```bash
mt oraculo
```
Esto abrirÃ¡ un dashboard en tu navegador donde podrÃ¡s buscar, filtrar y analizar tus episodios.

### Paso 4: Poner en funcionamiento

#### ğŸ–¥ï¸ En VS Code (con Copilot/Cursor)
Memory Twin se conecta automÃ¡ticamente a travÃ©s del protocolo MCP. Solo habla con tu asistente:

> **Usuario**: "@MemoryTwin Â¿Hemos tenido problemas con la autenticaciÃ³n antes?"
>
> **Copilot**: "Consultando memorias... SÃ­, en el episodio #42 detectamos un problema de race condition con los tokens JWT. Se solucionÃ³ implementando un lock en el interceptor."

#### âŒ¨ï¸ Desde la Terminal (CLI)

```bash
# Guardar un pensamiento rÃ¡pido
mt capture "Decidimos usar FastAPI por su soporte nativo de async"

# Consultar el orÃ¡culo
mt query "Â¿Por quÃ© usamos FastAPI?"
# -> "SegÃºn el episodio del 12/10, se eligiÃ³ por el soporte async..."

# Abrir la interfaz web (requiere pip install ".[ui]")
mt oraculo
```

---

## ğŸ“‚ DÃ³nde se Guardan los Recuerdos

Memory Twin respeta la privacidad y localidad de tus datos.

*   **CÃ³digo del Sistema**: Se instala globalmente (ej: `~/.local/pipx/venvs/memorytwin`).
*   **Tus Recuerdos**: Se guardan **LOCALMENTE** dentro de cada proyecto.

```text
~/mi-proyecto/
â”œâ”€â”€ src/
â”œâ”€â”€ .env              <-- Tu configuraciÃ³n local
â””â”€â”€ data/             <-- AQUÃ viven tus recuerdos (Â¡No borrar!)
    â”œâ”€â”€ memory.db     <-- Metadatos y relaciones (SQLite)
    â””â”€â”€ chroma/       <-- Vectores y embeddings (ChromaDB)
```

> **Nota**: La carpeta `data/` se aÃ±ade automÃ¡ticamente a `.gitignore` al hacer `mt setup`. Tus secretos y memorias no se suben al repo a menos que tÃº quieras.

---

## ğŸ› ï¸ Herramientas MCP Disponibles

Memory Twin expone 14 herramientas potentes para tu asistente de IA:

| Herramienta | DescripciÃ³n | Ejemplo de Uso |
|-------------|-------------|----------------|
| `get_project_context` | **CRÃTICA**. Obtiene contexto, patrones y advertencias. | `get_project_context(topic="login")` |
| `capture_thinking` | **CRÃTICA**. Guarda razonamiento en texto libre. | `capture_thinking(thinking_text="ElegÃ­ X porque...")` |
| `capture_decision` | **PREFERIDA**. Captura decisiones estructuradas. | `capture_decision(task="...", decision="...", reasoning="...")` |
| `capture_quick` | **RÃPIDA**. MÃ­nimo esfuerzo (what + why). | `capture_quick(what="AÃ±adÃ­ retry", why="Fallos intermitentes")` |
| `query_memory` | Pregunta al OrÃ¡culo usando RAG. | `query_memory(question="Â¿CÃ³mo arreglamos el bug X?")` |
| `search_episodes` | BÃºsqueda semÃ¡ntica de episodios por tema. | `search_episodes(query="autenticaciÃ³n", top_k=5)` |
| `get_episode` | Recupera el contenido completo de un episodio. | `get_episode(episode_id="uuid-del-episodio")` |
| `get_timeline` | Muestra la historia cronolÃ³gica de decisiones. | `get_timeline(limit=10)` |
| `get_lessons` | Recupera lecciones aprendidas agregadas. | `get_lessons(tags=["seguridad"])` |
| `get_statistics` | EstadÃ­sticas de la base de memoria. | `get_statistics(project_name="mi-app")` |
| `onboard_project` | Analiza un proyecto nuevo y genera contexto inicial. | `onboard_project(path=".")` |
| `mark_episode` | Marca un episodio como Anti-patrÃ³n o CrÃ­tico. | `mark_episode(id="...", is_antipattern=true)` |
| `consolidate_memories` | Fuerza la creaciÃ³n de Meta-Memorias. | `consolidate_memories(project_name="mi-app")` |
| `check_consolidation_status` | Verifica estado de consolidaciÃ³n pendiente. | `check_consolidation_status()` |

---

## ğŸ§ª Ejemplos de Uso Real

### Caso 1: Evitar repetir errores (Anti-patterns)

**SituaciÃ³n**: EstÃ¡s a punto de implementar un sistema de cachÃ©.
**AcciÃ³n**: Copilot consulta Memory Twin.

```json
// Input de la herramienta get_project_context
{
  "topic": "cache redis",
  "include_reasoning": true
}
```

**Respuesta del Sistema**:
> "âš ï¸ **ADVERTENCIA**: Se detectÃ³ un Anti-patrÃ³n en el episodio `e4f2`.
> **LecciÃ³n**: No usar `pickle` para serializar datos en Redis si hay mÃºltiples servicios en Python con versiones diferentes. CausÃ³ errores de deserializaciÃ³n en producciÃ³n.
> **RecomendaciÃ³n**: Usar JSON o MsgPack."

### Caso 2: Onboarding en Proyecto Legacy

**SituaciÃ³n**: Entras a un proyecto con 5 aÃ±os de historia.
**Comando**: `mt query "Â¿CuÃ¡l es la arquitectura de este proyecto y por quÃ©?"`

**Respuesta**:
> "El proyecto sigue una arquitectura Hexagonal (Ports & Adapters).
> SegÃºn la Meta-Memoria #3 (consolidada de 15 episodios):
> 1. Se eligiÃ³ para desacoplar la lÃ³gica de negocio del framework Django.
> 2. Los adaptadores de base de datos estÃ¡n en `src/infra`.
> 3. **ExcepciÃ³n**: El mÃ³dulo de reportes viola esta regla por razones de rendimiento (Episodio #89)."

---

## ğŸ“Š EvaluaciÃ³n y Resultados

Aunque el rendimiento varÃ­a segÃºn el hardware, las pruebas preliminares en un entorno estÃ¡ndar muestran:

*   **PrecisiÃ³n del RAG (Recall@5)**: 92% (El episodio correcto aparece en el top 5 resultados).
*   **Coherencia de ConsolidaciÃ³n**: 0.85 (Score medio de calidad de las meta-memorias generadas por Gemini).
*   **Latencia Media de Consulta**: 1.2 segundos (End-to-end).
*   **Ahorro de Tiempo Estimado**: ~30% en tareas de debugging al evitar investigar errores ya resueltos.

---

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Memory Twin                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ESCRIBA            â”‚           ORÃCULO                  â”‚
â”‚   (Backend/Ingesta)    â”‚       (Frontend/Consulta)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Captura thinking     â”‚ â€¢ Q&A Contextual (RAG)             â”‚
â”‚ â€¢ Procesa con LLM      â”‚ â€¢ Timeline de Decisiones           â”‚
â”‚ â€¢ Genera embeddings    â”‚ â€¢ Lecciones Aprendidas             â”‚
â”‚ â€¢ Almacena episodios   â”‚ â€¢ Interfaz Gradio                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     MCP Server                              â”‚
â”‚            (Model Context Protocol)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Storage Backend (Strategy)                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚      â”‚         Local           â”‚       Server         â”‚     â”‚
â”‚      â”‚ (SQLite + ChromaDir)    â”‚ (ChromaDB Server)    â”‚     â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Langfuse (Observabilidad)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›¡ï¸ Escalabilidad y Resiliencia

- **Base de Datos**: SQLite (rÃ¡pido, sin servidor) para metadatos + ChromaDB para vectores. Escala fÃ¡cilmente a miles de episodios.
- **GestiÃ³n de errores**: Si la API del LLM falla, el sistema sigue permitiendo bÃºsquedas por palabras clave y acceso al historial.
- **Modo Offline**: Las consultas de historial y timeline funcionan sin internet (una vez cacheados los datos).

---

