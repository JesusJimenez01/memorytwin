# Memory Twin - Configuration
# ============================

# =============================================================================
# LLM PROVIDER - Choose ONE of the options below
# =============================================================================

# Option A: OpenRouter (recommended - access to many free models)
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_key_here
LLM_PROVIDER=openrouter
LLM_MODEL=amazon/nova-2-lite-v1:free

# Free models on OpenRouter (updated Dec 2025):
# - amazon/nova-2-lite-v1:free (1M context, fast)
# - qwen/qwen3-coder:free (262K context, great for code)
# - tngtech/deepseek-r1t-chimera:free (164K context, reasoning)

# Option B: Google Gemini
# Get your key at: https://aistudio.google.com/apikey
# GOOGLE_API_KEY=your_google_api_key_here
# LLM_PROVIDER=google
# LLM_MODEL=gemini-2.0-flash

# =============================================================================
# OPTIONAL SETTINGS
# =============================================================================

# LLM Temperature (0.0 = deterministic, 1.0 = creative)
# LLM_TEMPERATURE=0.3

# Langfuse (Observability)
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_HOST=https://cloud.langfuse.com

# ================================
# Storage Backend Configuration
# ================================
# Options: "local", "chromadb_server", "supabase"
MEMORYTWIN_STORAGE_BACKEND=local

# Local Storage (default)
CHROMA_PERSIST_DIR=./data/chroma
SQLITE_DB_PATH=./data/memory.db

# ChromaDB Server (for shared/team usage)
CHROMADB_SERVER_HOST=localhost
CHROMADB_SERVER_PORT=8000

# Supabase (cloud PostgreSQL + pgvector)
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_KEY=your-anon-key

# MCP Server
MCP_SERVER_HOST=localhost
MCP_SERVER_PORT=8765

# Gradio Interface
GRADIO_SERVER_PORT=7860
GRADIO_SHARE=false
