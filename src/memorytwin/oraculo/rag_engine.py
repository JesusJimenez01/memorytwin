"""
Motor RAG para el Or√°culo
=========================

Implementa Retrieval-Augmented Generation sobre la base
de datos de memorias epis√≥dicas para responder preguntas
contextuales sobre decisiones t√©cnicas.

Ahora incluye soporte para MetaMemories (conocimiento consolidado).
"""

from typing import Optional

from memorytwin.config import get_llm_model, get_settings
from memorytwin.models import MemoryQuery, MemorySearchResult, MetaMemory, MetaMemorySearchResult
from memorytwin.escriba.storage import MemoryStorage
from memorytwin.observability import trace_access_memory, _get_langfuse, _is_disabled, flush_traces


# Prompt del sistema para el Or√°culo
ORACLE_SYSTEM_PROMPT = """Eres el Or√°culo del Memory Twin, un asistente especializado en responder preguntas sobre la historia t√©cnica y las decisiones de desarrollo de un proyecto de software.

Tu conocimiento proviene de dos fuentes:

1. **META-MEMORIAS**: Conocimiento consolidado de m√∫ltiples episodios relacionados. Representan patrones, lecciones y mejores pr√°cticas identificadas autom√°ticamente. Son m√°s confiables y generales.

2. **EPISODIOS**: Memorias individuales que documentan el razonamiento ("thinking") de asistentes de IA durante el desarrollo. Contienen:
   - La tarea o problema abordado
   - El contexto t√©cnico
   - La traza de razonamiento (alternativas consideradas, factores de decisi√≥n)
   - La soluci√≥n implementada
   - Lecciones aprendidas

INSTRUCCIONES:
1. Prioriza las META-MEMORIAS si est√°n disponibles (son conocimiento consolidado)
2. Complementa con EPISODIOS para detalles espec√≠ficos
3. Si la informaci√≥n no est√° en la memoria, indica que no hay registros sobre ese tema
4. Cita las fuentes (meta-memorias o episodios) cuando sea √∫til
5. Explica el "porqu√©" detr√°s de las decisiones, no solo el "qu√©"
6. Si hay lecciones aprendidas o mejores pr√°cticas relevantes, incl√∫yelas
7. S√© conciso pero completo
8. Usa formato Markdown para mejor legibilidad

FORMATO DE RESPUESTA:
- Respuesta directa a la pregunta
- Contexto relevante de las fuentes
- Lecciones aprendidas / mejores pr√°cticas aplicables
- Referencias a meta-memorias o episodios espec√≠ficos
"""


class RAGEngine:
    """
    Motor de Retrieval-Augmented Generation para consultas
    sobre memorias epis√≥dicas.
    """
    
    def __init__(
        self,
        storage: Optional[MemoryStorage] = None,
        api_key: Optional[str] = None
    ):
        """
        Inicializar el motor RAG.
        
        Args:
            storage: Almacenamiento de memoria (se crea uno si no se provee)
            api_key: DEPRECATED - ya no se usa, la API key se lee de config.
        """
        self.storage = storage or MemoryStorage()
        
        # Usar factory centralizada (temperatura un poco m√°s alta para respuestas creativas)
        self.model = get_llm_model(temperature=0.4, max_output_tokens=2048)

    @trace_access_memory
    async def query(
        self,
        question: str,
        project_name: Optional[str] = None,
        top_k: int = 5,
        include_meta_memories: bool = True
    ) -> dict:
        """
        Realizar una consulta RAG sobre las memorias.
        
        Busca primero en MetaMemories (conocimiento consolidado)
        y luego complementa con episodios individuales.
        
        Args:
            question: Pregunta del usuario
            project_name: Filtrar por proyecto espec√≠fico
            top_k: N√∫mero de resultados a recuperar
            include_meta_memories: Si incluir meta-memorias en la b√∫squeda
            
        Returns:
            Dict con respuesta, episodios usados, meta-memorias y metadatos
        """
        meta_results = []
        
        # Buscar meta-memorias primero (conocimiento consolidado)
        if include_meta_memories:
            meta_results = self.storage.search_meta_memories(
                query=question,
                project_name=project_name,
                top_k=min(3, top_k)  # M√°ximo 3 meta-memorias
            )
        
        # Buscar episodios relevantes
        memory_query = MemoryQuery(
            query=question,
            project_filter=project_name,
            top_k=top_k
        )
        
        search_results = self.storage.search_episodes(memory_query)
        
        if not search_results and not meta_results:
            return {
                "answer": "No encontr√© episodios de memoria ni conocimiento consolidado "
                         "relacionados con tu pregunta. "
                         "Es posible que este tema no haya sido documentado a√∫n.",
                "episodes_used": [],
                "meta_memories_used": [],
                "relevance_scores": [],
                "context_provided": False
            }
        
        # Construir contexto combinando meta-memorias y episodios
        context = self._build_combined_context(meta_results, search_results)
        
        # Generar respuesta
        answer = await self._generate_answer(question, context)
        
        return {
            "answer": answer,
            "episodes_used": [r.episode for r in search_results],
            "meta_memories_used": [r.meta_memory for r in meta_results],
            "relevance_scores": [r.relevance_score for r in search_results],
            "meta_relevance_scores": [r.relevance_score for r in meta_results],
            "context_provided": True
        }
    
    def query_sync(
        self,
        question: str,
        project_name: Optional[str] = None,
        top_k: int = 5
    ) -> dict:
        """Versi√≥n s√≠ncrona de query."""
        import asyncio
        return asyncio.run(self.query(question, project_name, top_k))
    
    def _build_context(self, results: list[MemorySearchResult]) -> str:
        """Construir contexto de episodios para el LLM."""
        context_parts = ["## EPISODIOS DE MEMORIA RELEVANTES\n"]
        
        for i, result in enumerate(results, 1):
            ep = result.episode
            
            context_parts.append(f"""
### Episodio {i} (Relevancia: {result.relevance_score:.0%})
- **ID**: {ep.id}
- **Fecha**: {ep.timestamp.strftime('%Y-%m-%d %H:%M')}
- **Tipo**: {ep.episode_type.value}
- **Proyecto**: {ep.project_name}
- **Asistente**: {ep.source_assistant}

**Tarea**: {ep.task}

**Contexto**: {ep.context}

**Razonamiento**:
{ep.reasoning_trace.raw_thinking}

**Alternativas consideradas**: {', '.join(ep.reasoning_trace.alternatives_considered) or 'No documentadas'}

**Factores de decisi√≥n**: {', '.join(ep.reasoning_trace.decision_factors) or 'No documentados'}

**Soluci√≥n**: {ep.solution_summary}

**Lecciones aprendidas**: {', '.join(ep.lessons_learned) or 'Ninguna documentada'}

**Tags**: {', '.join(ep.tags)}
---
""")
        
        return "\n".join(context_parts)
    
    def _build_combined_context(
        self,
        meta_results: list[MetaMemorySearchResult],
        episode_results: list[MemorySearchResult]
    ) -> str:
        """
        Construir contexto combinando meta-memorias y episodios.
        
        Las meta-memorias van primero (mayor prioridad).
        """
        context_parts = []
        
        # Primero las meta-memorias (conocimiento consolidado)
        if meta_results:
            context_parts.append("## META-MEMORIAS (Conocimiento Consolidado)\n")
            context_parts.append("*Estas son lecciones consolidadas de m√∫ltiples episodios relacionados.*\n")
            
            for i, result in enumerate(meta_results, 1):
                mm = result.meta_memory
                
                context_parts.append(f"""
### Meta-Memoria {i} (Relevancia: {result.relevance_score:.0%} | Confianza: {mm.confidence:.0%})
- **Patr√≥n**: {mm.pattern_summary}
- **Basado en**: {mm.episode_count} episodios
- **Tecnolog√≠as**: {', '.join(mm.technologies) or 'No especificadas'}

**Descripci√≥n del Patr√≥n**:
{mm.pattern}

**Lecciones Aprendidas**:
{chr(10).join(f'‚Ä¢ {l}' for l in mm.lessons) or '‚Ä¢ Ninguna documentada'}

**Mejores Pr√°cticas**:
{chr(10).join(f'‚Ä¢ {p}' for p in mm.best_practices) or '‚Ä¢ Ninguna documentada'}

**Anti-patrones a Evitar**:
{chr(10).join(f'‚Ä¢ {a}' for a in mm.antipatterns) or '‚Ä¢ Ninguno documentado'}

**Excepciones/Casos Especiales**:
{chr(10).join(f'‚Ä¢ {e}' for e in mm.exceptions) or '‚Ä¢ Ninguna documentada'}

**Contextos Aplicables**: {', '.join(mm.contexts) or 'General'}
---
""")
        
        # Luego los episodios individuales
        if episode_results:
            context_parts.append("\n## EPISODIOS INDIVIDUALES\n")
            context_parts.append("*Detalles espec√≠ficos de decisiones particulares.*\n")
            context_parts.append(self._build_context(episode_results))
        
        return "\n".join(context_parts)
    
    async def _generate_answer(self, question: str, context: str) -> str:
        """Generar respuesta usando el LLM con observabilidad."""
        prompt = f"""## CONTEXTO DE MEMORIA
{context}

## PREGUNTA DEL USUARIO
{question}

## TU RESPUESTA (en espa√±ol, usando Markdown)
"""
        
        # Trazar generaci√≥n del LLM
        langfuse = _get_langfuse() if not _is_disabled() else None
        generation = None
        
        try:
            if langfuse:
                generation = langfuse.start_as_current_generation(
                    name="üß† Or√°culo - Respuesta LLM",
                    model=get_settings().llm_model,
                    model_parameters={"temperature": 0.4, "max_output_tokens": 2048},
                    input={"question": question, "context_length": len(context)}
                ).__enter__()
            
            # Llamar al LLM (interfaz unificada)
            response = await self.model.generate_async(
                [
                    {"role": "user", "parts": [ORACLE_SYSTEM_PROMPT]},
                    {"role": "model", "parts": ["Entendido. Estoy listo para responder preguntas sobre la memoria t√©cnica del proyecto bas√°ndome √∫nicamente en los episodios proporcionados."]},
                    {"role": "user", "parts": [prompt]}
                ]
            )
            
            answer = response.text
            
            if generation:
                generation.update(output=answer[:1000])  # Limitar output para no saturar
            
            return answer
            
        finally:
            if generation:
                try:
                    generation.end()
                except Exception:
                    pass
            if langfuse:
                flush_traces()
    
    def get_timeline(
        self,
        project_name: Optional[str] = None,
        limit: int = 50
    ) -> list:
        """
        Obtener timeline de decisiones para visualizaci√≥n.
        
        Args:
            project_name: Filtrar por proyecto
            limit: N√∫mero m√°ximo de episodios
            
        Returns:
            Lista de episodios ordenados cronol√≥gicamente
        """
        episodes = self.storage.get_timeline(
            project_name=project_name,
            limit=limit
        )
        
        # Formatear para visualizaci√≥n
        timeline = []
        for ep in episodes:
            timeline.append({
                "id": str(ep.id),
                "timestamp": ep.timestamp.isoformat(),
                "date": ep.timestamp.strftime("%Y-%m-%d"),
                "time": ep.timestamp.strftime("%H:%M"),
                "task": ep.task,
                "type": ep.episode_type.value,
                "summary": ep.solution_summary,
                "tags": ep.tags,
                "assistant": ep.source_assistant,
                "success": ep.success
            })
            
        return timeline
    
    def get_lessons(
        self,
        project_name: Optional[str] = None,
        tags: Optional[list[str]] = None
    ) -> list:
        """
        Obtener lecciones aprendidas agregadas.
        
        Args:
            project_name: Filtrar por proyecto
            tags: Filtrar por tags
            
        Returns:
            Lista de lecciones con contexto
        """
        return self.storage.get_lessons_learned(
            project_name=project_name,
            tags=tags
        )
    
    def get_statistics(self, project_name: Optional[str] = None) -> dict:
        """Obtener estad√≠sticas de la memoria."""
        return self.storage.get_statistics(project_name)
